"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from datetime import datetime
from typing import Dict, Any, Optional, List, Union

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

# Configuration constants
API_KEY = "YOUR-API_KEY"
BASE_URL = "https://www.beautylish.com"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class Product:
    name: str = ""
    price: float = 0.0
    url: str = ""
    currency: str = "USD"
    availability: str = "in_stock"
    brand: str = ""
    images: List[str] = field(default_factory=list)
    description: str = ""
    productId: str = ""
    category: str = ""
    aggregateRating: Optional[Any] = None
    reviews: List[Any] = field(default_factory=list)
    features: List[str] = field(default_factory=list)
    specifications: List[Any] = field(default_factory=list)
    videos: List[Any] = field(default_factory=list)
    seller: Optional[Any] = None
    serialNumbers: List[Any] = field(default_factory=list)
    preDiscountPrice: Optional[float] = None

@dataclass
class ScrapedData:
    searchMetadata: Dict[str, Any] = field(default_factory=dict)
    products: List[Product] = field(default_factory=list)
    breadcrumbs: Optional[Any] = None
    pagination: Optional[Any] = None
    recommendations: Optional[Any] = None
    relatedSearches: Optional[Any] = None
    sponsoredProducts: Optional[Any] = None

class DataPipeline:
    def __init__(self, jsonl_filename: str):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: Any) -> bool:
        item_key = str(input_data)
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        data_dict = asdict(scraped_data)
        if not self.is_duplicate(data_dict):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(data_dict, ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved {len(scraped_data.products)} products to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str or url_str == "None":
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    if url_str.startswith("/"):
        return f"{BASE_URL}{url_str}"
    return f"{BASE_URL}/{url_str}"

def parse_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    try:
        # Remove commas and non-numeric chars except decimal
        cleaned = price_text.replace(",", "")
        match = re.search(r'([\d]+\.?\d*)', cleaned)
        if match:
            return float(match.group(1))
    except (ValueError, TypeError):
        pass
    return 0.0

def detect_currency(price_text: str) -> str:
    if not price_text:
        return "USD"
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Converted ExtractData function from Go to Python using Playwright."""
    try:
        html_content = await page.content()
        
        # 1. Extract JSON-LD
        json_ld_data = []
        scripts = await page.locator("script[type='application/ld+json']").all()
        for s in scripts:
            try:
                text = await s.inner_text()
                data = json.loads(text)
                if isinstance(data, dict):
                    json_ld_data.append(data)
                elif isinstance(data, list):
                    json_ld_data.extend([i for i in data if isinstance(i, dict)])
            except Exception:
                continue

        # 2. Extract NEXT_DATA
        next_data = {}
        next_script = page.locator("script#__NEXT_DATA__")
        if await next_script.count() > 0:
            try:
                next_data = json.loads(await next_script.inner_text())
            except Exception:
                pass

        output_data = ScrapedData()
        
        # Search Metadata Extraction
        search_input = page.locator("input.ocnSearch_input")
        query = "hair shampoo"
        if await search_input.count() > 0:
            val = await search_input.first.get_attribute("value")
            if val:
                query = val
        
        output_data.searchMetadata = {
            "query": query,
            "resultsDisplayed": 0,
            "searchType": "keyword",
            "searchUrl": await page.url(),
            "totalResults": 0
        }

        # Strategy A: NEXT_DATA recursive search
        def find_products(data: Any) -> Optional[List[Any]]:
            if not isinstance(data, dict):
                return None
            keys_to_check = ["searchResults", "products", "items", "results", "entities", "catalog", "hits"]
            for k, v in data.items():
                if k in keys_to_check and isinstance(v, list) and len(v) > 0:
                    return v
                if isinstance(v, dict):
                    found = find_products(v)
                    if found:
                        return found
            return None

        products_list = find_products(next_data)
        if products_list:
            for item in products_list:
                if isinstance(item, dict):
                    p_name = item.get("name") or item.get("title", "")
                    if p_name:
                        p = Product(
                            name=str(p_name),
                            price=parse_price(str(item.get("price", "0"))),
                            url=make_absolute_url(str(item.get("url", ""))),
                            currency="USD",
                            availability="in_stock"
                        )
                        if item.get("image"):
                            p.images = [make_absolute_url(str(item.get("image")))]
                        output_data.products.append(p)

        # Strategy B: __INITIAL_STATE__ and others
        if not output_data.products:
            scripts = await page.locator("script").all()
            for s in scripts:
                content = await s.inner_text()
                if any(x in content for x in ["__INITIAL_STATE__", "__PRELOADED_STATE__", "window.data", "window.BEAUTYLISH"]):
                    match = re.search(r'(?s)\{.*\}', content)
                    if match:
                        try:
                            state = json.loads(match.group(0))
                            state_prods = find_products(state)
                            if state_prods:
                                for item in state_prods:
                                    if isinstance(item, dict):
                                        p_name = item.get("name") or item.get("title", "")
                                        if p_name:
                                            p = Product(
                                                name=str(p_name),
                                                price=parse_price(str(item.get("price", "0"))),
                                                url=make_absolute_url(str(item.get("url", "")))
                                            )
                                            output_data.products.append(p)
                        except Exception:
                            continue

        # Strategy C: DOM Scraping (Fallbacks)
        if not output_data.products:
            selectors = [
                ".product-item", "[itemtype*='Product']", ".product-card", 
                ".product-grid-item", ".product-list-item", "a[href*='/p/']", 
                ".item-card", ".product-grid-item__link", ".product-results > div", 
                ".search-results-container > div", ".product-list > div"
            ]
            combined_selector = ", ".join(selectors)
            items = await page.locator(combined_selector).all()
            
            for i, item in enumerate(items):
                ld = json_ld_data[i] if i < len(json_ld_data) else None
                
                # Name
                name = ""
                if ld and ld.get("name"):
                    name = ld["name"]
                else:
                    name_el = item.locator(".product-name, [itemprop='name'], .name, h3, h4, .product-grid-item__name, .item-name, .title, .product-title").first
                    if await name_el.count() > 0:
                        name = (await name_el.inner_text()).strip()
                
                if not name:
                    continue

                # Price
                price = 0.0
                if ld and ld.get("offers"):
                    offers = ld["offers"]
                    if isinstance(offers, dict):
                        price = parse_price(str(offers.get("price", 0)))
                else:
                    price_el = item.locator(".price, [itemprop='price'], .product-price, .product-grid-item__price, .item-price, .amount, .price-display").first
                    if await price_el.count() > 0:
                        price = parse_price(await price_el.inner_text())

                # URL
                link = ""
                if ld and ld.get("url"):
                    link = make_absolute_url(ld["url"])
                else:
                    # Check if current element is <a>
                    tag_name = await item.evaluate("el => el.tagName")
                    if tag_name == "A":
                        href = await item.get_attribute("href")
                        link = make_absolute_url(href)
                    else:
                        anchor = item.locator("a").first
                        if await anchor.count() > 0:
                            href = await anchor.get_attribute("href")
                            link = make_absolute_url(href)

                output_data.products.append(Product(name=name, price=price, url=link))

        return output_data

    except Exception as e:
        logger.error(f"Error in extract_data: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            logger.info(f"Navigating to {url} (Try {tries+1})")
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            
            # Smart Wait for content
            try:
                await page.wait_for_selector(".product-item, .product-grid-item, .product-list", timeout=10000)
            except:
                pass
            
            scraped_data = await extract_data(page)
            
            if scraped_data and scraped_data.products:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No products found on {url}")
                # Manual fallback retry if page didn't load items
                await page.wait_for_timeout(2000)
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

def generate_output_filename() -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_search_page_scraper_data_{timestamp}.jsonl"

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/shop/browse?utm_source=internal_search&utm_campaign=Search+Beautylish+Products&q=hair+shampoo",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")