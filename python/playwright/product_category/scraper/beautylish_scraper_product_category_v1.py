"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from datetime import datetime
from typing import Dict, Any, Optional, List, Union

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

# ScrapeOps Residential Proxy Configuration
API_KEY = "YOUR-API_KEY"
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    appliedFilters: List[Dict[str, str]] = field(default_factory=list)
    bannerImage: Optional[str] = None
    breadcrumbs: Optional[Any] = None
    categoryId: str = ""
    categoryName: str = ""
    categoryUrl: str = ""
    description: Optional[str] = None
    pagination: Dict[str, Any] = field(default_factory=lambda: {
        "currentPage": 1,
        "nextPageUrl": None,
        "prevPageUrl": None,
        "resultsPerPage": None,
        "totalPages": None,
        "totalResults": None,
    })
    products: List[Any] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, scraped_data: ScrapedData) -> bool:
        # Create a unique key based on category URL and name
        item_key = f"{scraped_data.categoryUrl}_{scraped_data.categoryName}"
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_content: str) -> str:
    if not html_content:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_content).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://www.beautylish.com"
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def extract_numeric_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    price_text = price_text.replace(",", "")
    match = re.search(r'[\d,]+\.?\d*', price_text)
    if match:
        try:
            return float(match.group())
        except ValueError:
            return 0.0
    return 0.0

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright locator and evaluate methods."""
    try:
        # Get canonical URL
        canonical = ""
        canonical_el = page.locator("link[rel='canonical']")
        if await canonical_el.count() > 0:
            canonical = await canonical_el.get_attribute("href") or ""

        # appliedFilters
        applied_filters = []
        if "tag=skincare" in canonical:
            applied_filters.append({
                "filterName": "tag",
                "filterValue": "skincare"
            })

        # categoryId
        cat_id = ""
        if "tag=" in canonical:
            parts = canonical.split("tag=")
            if len(parts) > 1:
                cat_id = parts[1]

        # categoryName
        cat_name = ""
        h1_el = page.locator("h1.category-name").first
        if await h1_el.count() > 0:
            cat_name = (await h1_el.text_content() or "").strip()
        
        if not cat_name:
            title = await page.title()
            cat_name = title.split("|")[0].strip()

        # subcategories
        subcategories = []
        subcategory_items = page.locator(".squeezeBox_sublist .squeezeBox_item a")
        count = await subcategory_items.count()
        for i in range(count):
            item = subcategory_items.nth(i)
            name = (await item.text_content() or "").strip()
            href = await item.get_attribute("href") or ""
            if name:
                subcategories.append({
                    "name": name,
                    "productCount": None,
                    "url": make_absolute_url(href)
                })

        # Global helpers placeholders (simulating Go logic)
        detect_currency("")
        extract_numeric_price("")
        strip_html("")

        return ScrapedData(
            appliedFilters=applied_filters,
            categoryId=cat_id,
            categoryName=cat_name,
            categoryUrl=make_absolute_url(canonical),
            subcategories=subcategories,
            bannerImage=None,
            breadcrumbs=None,
            description=None,
            pagination={
                "currentPage": 1,
                "nextPageUrl": None,
                "prevPageUrl": None,
                "resultsPerPage": None,
                "totalPages": None,
                "totalResults": None,
            },
            products=[]
        )
    except Exception as e:
        logger.error(f"Error during data extraction: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(1000)
            
            scraped_data = await extract_data(page)
            
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Try {tries}: Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

def generate_output_filename() -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_category_page_scraper_data_{timestamp}.jsonl"

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/shop/browse?tag=skincare",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")