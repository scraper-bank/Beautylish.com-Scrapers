"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from datetime import datetime
from typing import Dict, Any, Optional, List, Union

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScrapedData:
    aggregateRating: Dict[str, Optional[float]] = field(default_factory=lambda: {
        "bestRating": None,
        "ratingValue": None,
        "reviewCount": None,
        "worstRating": None
    })
    availability: str = ""
    brand: str = ""
    category: Any = None
    currency: str = ""
    description: str = ""
    features: List[str] = field(default_factory=list)
    images: List[Dict[str, str]] = field(default_factory=list)
    name: str = ""
    preDiscountPrice: Any = None
    price: float = 0.0
    productId: Any = None
    reviews: List[Any] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": "Beautylish",
        "rating": None,
        "url": "https://www.beautylish.com"
    })
    serialNumbers: List[Any] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    url: str = ""
    videos: List[Dict[str, str]] = field(default_factory=list)


class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.url or str(asdict(input_data))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")


async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright, replicating the Go logic."""
    try:
        html_content = await page.content()
        
        def make_absolute_url(url_str: str) -> str:
            if not url_str:
                return ""
            url_str = url_str.strip()
            if url_str.startswith(("http://", "https://")):
                return url_str
            if url_str.startswith("//"):
                return f"https:{url_str}"
            domain = "https://www.beautylish.com"
            if url_str.startswith("/"):
                return f"{domain}{url_str}"
            return f"{domain}/{url_str}"

        def detect_currency(price_text: str) -> str:
            price_text = price_text.upper()
            currency_map = {
                "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
                "EUR": "EUR", "€": "EUR",
                "GBP": "GBP", "£": "GBP", "GB£": "GBP",
                "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
                "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
                "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
                "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
                "CHF": "CHF", "FR.": "CHF",
                "SEK": "SEK", "KR": "SEK",
                "NZD": "NZD", "NZ$": "NZD",
            }
            for code, currency in currency_map.items():
                if code in price_text:
                    return currency
            return "USD"

        def parse_price(price_str: str) -> float:
            if not price_str:
                return 0.0
            cleaned = price_str.replace(",", "")
            match = re.search(r"[\d,]+\.?\d*", cleaned)
            if match:
                try:
                    return float(match.group())
                except ValueError:
                    return 0.0
            return 0.0

        # Extract JSON-LD
        json_ld_scripts = await page.locator("script[type='application/ld+json']").all_text_contents()
        json_data = None
        for script in json_ld_scripts:
            try:
                data = json.loads(script)
                if isinstance(data, dict) and data.get("@type", "").lower() == "product":
                    json_data = data
                    break
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and item.get("@type", "").lower() == "product":
                            json_data = item
                            break
            except Exception:
                continue

        result = ScrapedData()

        # 1. aggregateRating
        if json_data and "aggregateRating" in json_data:
            ar = json_data["aggregateRating"]
            try:
                if "ratingValue" in ar:
                    result.aggregateRating["ratingValue"] = float(ar["ratingValue"])
                if "reviewCount" in ar:
                    result.aggregateRating["reviewCount"] = float(ar["reviewCount"])
            except (ValueError, TypeError):
                pass

        # 2. availability
        availability = ""
        if json_data and "offers" in json_data:
            offers = json_data["offers"]
            if isinstance(offers, dict):
                av = offers.get("availability", "")
                if "InStock" in av:
                    availability = "in_stock"
                elif "OutOfStock" in av:
                    availability = "out_of_stock"
        
        if not availability:
            status = await page.locator(".availability").first.get_attribute("data-status")
            if status == "in_stock":
                availability = "in_stock"
        result.availability = availability

        # 3. brand
        brand = ""
        if json_data and "brand" in json_data:
            b = json_data["brand"]
            brand = b.get("name", "") if isinstance(b, dict) else ""
        if not brand:
            brand_el = page.locator(".product-brand").first
            brand = (await brand_el.inner_text()).strip() if await brand_el.count() > 0 else ""
        result.brand = brand

        # 5. currency
        currency = ""
        if json_data and "offers" in json_data:
            offers = json_data["offers"]
            if isinstance(offers, dict):
                currency = offers.get("priceCurrency", "")
        if not currency:
            price_el = page.locator(".product-price").first
            currency = detect_currency(await price_el.inner_text()) if await price_el.count() > 0 else "USD"
        result.currency = currency

        # 6. description
        if json_data:
            result.description = json_data.get("description", "")

        # 7. features
        h5_elements = await page.locator("h5").all()
        for h5 in h5_elements:
            text = await h5.inner_text()
            if "Why It’s Special" in text or "Key Ingredients" in text:
                # Find the next UL sibling after the H5
                li_elements = await page.evaluate_handle(
                    "(h5) => Array.from(h5.nextElementSibling.tagName === 'UL' ? h5.nextElementSibling.querySelectorAll('li') : [])",
                    h5
                )
                li_texts = await li_elements.json_value()
                # If evaluate_handle approach is complex, use locator relative logic:
                # Note: Playwright doesn't have a direct "nextFiltered", so we use JS.
                items = await page.evaluate("""(h5) => {
                    let next = h5.nextElementSibling;
                    while(next && next.tagName !== 'UL') next = next.nextElementSibling;
                    return next ? Array.from(next.querySelectorAll('li')).map(li => li.innerText.trim()) : [];
                }""", h5)
                result.features.extend(items)

        # 8. images
        detailed_images = []
        seen_urls = {}

        # Strategy 1: Carousel
        img_locators = await page.locator("div[data-section='product-carousel'] img, .product-media img").all()
        for img in img_locators:
            src = await img.get_attribute("src")
            if not src:
                continue
            
            # Beautylish URL Transformation
            full_res_src = re.sub(r"p_\d+x\d+[^/.]*", "zb_p", src)
            abs_url = make_absolute_url(full_res_src)
            alt = (await img.get_attribute("alt") or "").strip()

            lower_url = abs_url.lower()
            if any(x in lower_url for x in ["afterpay", "sezzle", "bl_logo", "icon", ".svg"]):
                continue

            if abs_url:
                if abs_url not in seen_urls:
                    seen_urls[abs_url] = len(detailed_images)
                    detailed_images.append({"url": abs_url, "alt_text": alt})
                else:
                    idx = seen_urls[abs_url]
                    current_alt = detailed_images[idx]["alt_text"]
                    if len(alt) > len(current_alt) or ("view" in alt.lower() and "view" not in current_alt.lower()):
                        detailed_images[idx]["alt_text"] = alt

        # Strategy 2: JSON-LD Fallback
        if len(detailed_images) < 2 and json_data and "image" in json_data:
            img_val = json_data["image"]
            imgs_to_process = [img_val] if isinstance(img_val, str) else (img_val if isinstance(img_val, list) else [])
            for u in imgs_to_process:
                if isinstance(u, str):
                    abs_url = make_absolute_url(u)
                    if abs_url and abs_url not in seen_urls:
                        seen_urls[abs_url] = len(detailed_images)
                        detailed_images.append({"url": abs_url, "alt_text": ""})

        result.images = detailed_images

        # 9. name
        name = ""
        if json_data:
            name = json_data.get("name", "")
        if not name:
            name_el = page.locator("h1.product-name").first
            name = (await name_el.inner_text()).strip() if await name_el.count() > 0 else ""
        result.name = name

        # 11. price
        price = 0.0
        if json_data and "offers" in json_data:
            offers = json_data["offers"]
            if isinstance(offers, dict) and "price" in offers:
                price = parse_price(str(offers["price"]))
        
        if price == 0:
            price_el = page.locator(".product-price").first
            if await price_el.count() > 0:
                p_attr = await price_el.get_attribute("data-price")
                price = parse_price(p_attr) if p_attr else 0.0
        result.price = price

        # 14. seller logic
        seller = {"name": "Beautylish", "rating": None, "url": "https://www.beautylish.com"}
        seller_elem = page.locator(".seller-name, .seller-info a, [itemprop='seller']").first
        if await seller_elem.count() > 0:
            found_name = (await seller_elem.inner_text()).strip()
            found_url = await seller_elem.get_attribute("href")
            if found_name:
                seller["name"] = found_name
                if found_url:
                    seller["url"] = make_absolute_url(found_url)

        if (seller["name"] == "Beautylish" or not seller["name"]) and json_data:
            offers = json_data.get("offers")
            if isinstance(offers, dict):
                js_seller = offers.get("seller")
                if isinstance(js_seller, dict):
                    js_name = js_seller.get("name")
                    if js_name:
                        seller["name"] = js_name.strip()
                        js_url = js_seller.get("url")
                        if js_url:
                            seller["url"] = make_absolute_url(js_url)

        if seller["name"] == "Beautylish" or not seller.get("name"):
            seller["name"] = "Beautylish"
            seller["url"] = "https://www.beautylish.com"
        
        seller["url"] = make_absolute_url(seller["url"])
        result.seller = seller

        # 16. specifications
        specs = []
        h5_all = await page.locator("h5").all()
        for h5 in h5_all:
            if "Other Details" in await h5.inner_text():
                items = await page.evaluate("""(h5) => {
                    let next = h5.nextElementSibling;
                    while(next && next.tagName !== 'UL') next = next.nextElementSibling;
                    if(!next) return [];
                    return Array.from(next.querySelectorAll('li')).map(li => li.innerText.trim());
                }""", h5)
                for txt in items:
                    if ":" in txt:
                        parts = txt.split(":", 1)
                        specs.append({"key": parts[0].strip(), "value": parts[1].strip()})
                    else:
                        specs.append({"key": txt, "value": "Yes"})
        result.specifications = specs

        # 17. url
        canonical = await page.locator("link[rel='canonical']").get_attribute("href")
        result.url = make_absolute_url(canonical) if canonical else page.url

        # 18. videos
        videos = []
        seen_video_urls = set()
        
        iframes = await page.locator("iframe").all()
        for iframe in iframes:
            src = await iframe.get_attribute("src") or await iframe.get_attribute("data-src")
            if not src:
                continue
            
            final_url = make_absolute_url(src)
            lower_url = final_url.lower()
            
            is_video_host = any(x in lower_url for x in ["youtube.com", "youtu.be", "vimeo.com"])
            is_blacklisted = any(x in lower_url for x in ["attentive", "attn.tv", "doubleclick", "googleads", "facebook", "analytics", "adsystem"])
            
            if is_video_host and not is_blacklisted:
                if final_url not in seen_video_urls:
                    videos.append({"url": final_url})
                    seen_video_urls.add(final_url)

        if not videos:
            anchors = await page.locator("a[href*='youtube.com'], a[href*='youtu.be'], a[href*='vimeo.com']").all()
            for a in anchors:
                href = await a.get_attribute("href")
                if href:
                    final_url = make_absolute_url(href)
                    if final_url not in seen_video_urls:
                        videos.append({"url": final_url})
                        seen_video_urls.add(final_url)
        result.videos = videos

        return result

    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None


async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            # Optimization: Block unnecessary resources
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"] and "zb_p" not in request.url:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            await page.wait_for_timeout(2000) # Ensure JS loads content
            
            scraped_data = await extract_data(page)
            
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page:
                await page.close()
            if context:
                await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")


async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()


if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/s/vital-proteins-vanilla-collagen-creamer-10-6-oz",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")