"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    aggregateRating: Dict[str, Any] = field(default_factory=dict)
    availability: str = ""
    brand: str = ""
    category: Optional[str] = None
    currency: str = "USD"
    description: str = ""
    features: List[str] = field(default_factory=list)
    images: List[Dict[str, str]] = field(default_factory=list)
    name: str = ""
    preDiscountPrice: Optional[float] = None
    price: Optional[float] = None
    productId: str = ""
    reviews: List[Any] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=dict)
    serialNumbers: List[Any] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    url: str = ""
    videos: List[Dict[str, str]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.productId or input_data.url
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    return re.sub(r'<[^>]*>', ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    domain = "https://www.beautylish.com"
    return urljoin(domain, url_str)

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def is_image_url(url: str) -> bool:
    lower = url.lower()
    return any(ext in lower for ext in [".jpg", ".jpeg", ".png", ".webp"])

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    try:
        json_data = {}
        scripts = soup.find_all("script", type="application/ld+json")
        for script in scripts:
            try:
                data = json.loads(script.get_text())
                if isinstance(data, dict) and data.get("@type", "").lower() == "product":
                    json_data = data
                    break
            except (json.JSONDecodeError, TypeError):
                continue

        # aggregateRating
        rating_obj = {
            "bestRating": None,
            "ratingValue": None,
            "reviewCount": None,
            "worstRating": None
        }
        if json_data and "aggregateRating" in json_data:
            agg = json_data["aggregateRating"]
            val = agg.get("ratingValue")
            if val is not None:
                try:
                    rating_obj["ratingValue"] = float(val)
                except (ValueError, TypeError): pass
            
            count = agg.get("reviewCount")
            if count is not None:
                try:
                    rating_obj["reviewCount"] = int(float(count))
                except (ValueError, TypeError): pass
        
        # availability
        availability = ""
        if json_data and "offers" in json_data:
            offers = json_data["offers"]
            avail = offers.get("availability")
            if isinstance(avail, str):
                if "InStock" in avail:
                    availability = "in_stock"
                elif "OutOfStock" in avail:
                    availability = "out_of_stock"
        
        if not availability:
            if soup.select_one(".product-price"):
                availability = "in_stock"

        # brand
        brand_name = ""
        if json_data and "brand" in json_data:
            b = json_data["brand"]
            if isinstance(b, dict):
                brand_name = b.get("name", "")
            elif isinstance(b, str):
                brand_name = b
        
        if not brand_name:
            brand_el = soup.select_one(".product-brand")
            if brand_el:
                brand_name = brand_el.get_text(strip=True)

        # currency
        currency = "USD"
        if json_data and "offers" in json_data:
            currency = json_data["offers"].get("priceCurrency", "USD")
        else:
            price_el = soup.select_one(".product-price")
            if price_el:
                currency = detect_currency(price_el.get_text())

        # description
        description = json_data.get("description", "") if json_data else ""

        # features
        features = []
        for h5 in soup.find_all("h5"):
            header_text = h5.get_text(strip=True)
            if header_text in ["Why It’s Special", "Key Ingredients"]:
                ul = h5.find_next("ul")
                if ul:
                    for li in ul.find_all("li"):
                        features.append(li.get_text(strip=True))

        # images
        image_list = []
        seen_urls = set()
        
        selectors = "div[data-section='product-carousel'] img, .product-media img, .main-image img"
        for img_tag in soup.select(selectors):
            src = img_tag.get("src") or img_tag.get("data-src")
            if src and is_image_url(src):
                high_res_url = src.replace("p_85x85", "zb_p") if "p_85x85" in src else src
                abs_url = make_absolute_url(high_res_url)
                if abs_url not in seen_urls:
                    image_list.append({
                        "url": abs_url,
                        "altText": img_tag.get("alt", "").strip()
                    })
                    seen_urls.add(abs_url)

        if not image_list and json_data and "image" in json_data:
            img_val = json_data["image"]
            if isinstance(img_val, str):
                image_list.append({"url": make_absolute_url(img_val), "altText": ""})
            elif isinstance(img_val, list):
                for item in img_val:
                    if isinstance(item, str):
                        image_list.append({"url": make_absolute_url(item), "altText": ""})

        # name
        name = json_data.get("name", "") if json_data else ""
        if not name:
            name_el = soup.select_one("h1.product-name")
            if name_el:
                name = name_el.get_text(strip=True)

        # price
        price = None
        if json_data and "offers" in json_data:
            p_val = json_data["offers"].get("price")
            if p_val is not None:
                try:
                    price = float(p_val)
                except (ValueError, TypeError): pass
        
        if price is None:
            price_el = soup.select_one(".product-price")
            if price_el:
                price_attr = price_el.get("data-price")
                if price_attr:
                    try:
                        price = float(price_attr)
                    except (ValueError, TypeError): pass
                else:
                    price_text = price_el.get_text()
                    match = re.search(r'[\d.]+', price_text)
                    if match:
                        try:
                            price = float(match.group())
                        except (ValueError, TypeError): pass

        # productId
        product_id = ""
        if json_data:
            product_id = json_data.get("sku") or json_data.get("mpn") or ""
        
        if not product_id:
            can_link = soup.select_one("link[rel='canonical']")
            if can_link and can_link.get("href"):
                product_id = can_link.get("href").rstrip("/").split("/")[-1]
        
        if not product_id:
            video_el = soup.select_one("[data-video-url]")
            if video_el:
                v_url = video_el.get("data-video-url", "")
                v_match = re.search(r'/v/([^/]+)/', v_url)
                if v_match:
                    product_id = v_match.group(1)

        # seller
        seller_name = ""
        seller_url = ""
        seller_el = soup.select_one(".seller-info .seller-name, .seller-name")
        if seller_el:
            seller_name = seller_el.get_text(strip=True)
            seller_url = seller_el.get("href", "")
        
        if not seller_name:
            html_tag = soup.find("html")
            canon_link = soup.select_one("link[rel='canonical']")
            canon_val = canon_link.get("href", "") if canon_link else ""
            if (html_tag and html_tag.get("data-theme") == "beautylish") or "beautylish.com" in canon_val:
                seller_name = "Beautylish"
                seller_url = "https://www.beautylish.com"
        
        seller_obj = {
            "name": seller_name,
            "url": make_absolute_url(seller_url),
            "rating": None
        }

        # specifications
        specs = []
        for h5 in soup.find_all("h5"):
            if h5.get_text(strip=True) == "Other Details":
                ul = h5.find_next("ul")
                if ul:
                    for li in ul.find_all("li"):
                        text = li.get_text(strip=True)
                        if ":" in text:
                            parts = text.split(":", 1)
                            specs.append({"key": parts[0].strip(), "value": parts[1].strip()})
                        else:
                            specs.append({"key": text, "value": "Yes"})

        # url
        canon = soup.select_one("link[rel='canonical']")
        final_url = make_absolute_url(canon.get("href", "")) if canon else ""

        # videos
        videos = []
        v_seen = set()
        v_elements = soup.select(".video-container, [data-video-url], video source, video")
        for v_el in v_elements:
            raw_url = v_el.get("data-video-url") or v_el.get("src") or v_el.get("data-src")
            if not raw_url:
                continue
            raw_url = raw_url.strip()
            
            if raw_url.startswith(("http://", "https://")):
                v_final = raw_url
            elif raw_url.startswith("//"):
                v_final = f"https:{raw_url}"
            else:
                v_final = make_absolute_url(raw_url)
            
            if v_final and v_final not in v_seen:
                videos.append({"url": v_final})
                v_seen.add(v_final)

        return ScrapedData(
            aggregateRating=rating_obj,
            availability=availability,
            brand=brand_name,
            currency=currency,
            description=description,
            features=features,
            images=image_list,
            name=name,
            price=price,
            productId=product_id,
            seller=seller_obj,
            specifications=specs,
            url=final_url,
            videos=videos
        )
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }
    tries = 0
    success = False
    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                if scraped_data:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/s/vital-proteins-vanilla-collagen-creamer-10-6-oz",
    ]
    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")