"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_search_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    products: List[Dict[str, Any]] = field(default_factory=list)
    searchMetadata: Dict[str, Any] = field(default_factory=dict)
    breadcrumbs: Optional[List[Any]] = None
    pagination: Optional[Dict[str, Any]] = None
    recommendations: Optional[List[Any]] = None
    relatedSearches: Optional[List[Any]] = None
    sponsoredProducts: Optional[List[Any]] = None

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        # Use products list string representation as a simple heuristic for duplication
        item_key = hash(str(input_data.products))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate dataset found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    return re.sub(r'<[^>]*>', ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://www.beautylish.com"
    return urljoin(domain, url_str)

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for key, val in currency_map.items():
        if key in price_text:
            return val
    return "USD"

def extract_numeric_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    cleaned = price_text.replace(",", "")
    match = re.search(r'([\d,]+\.?\d*)', cleaned)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return 0.0
    return 0.0

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    products = []

    # Strategy 1: Parse __NEXT_DATA__
    next_data_script = soup.find("script", id="__NEXT_DATA__")
    if next_data_script:
        try:
            next_data = json.loads(next_data_script.string)
            
            def find_products_recursive(data: Any) -> Optional[List[Any]]:
                if not isinstance(data, dict):
                    return None
                
                keys_to_check = ["products", "results", "items", "itemListElement", "initialState", "search", "browse"]
                for key in keys_to_check:
                    val = data.get(key)
                    if key in ["initialState", "search", "browse"] and isinstance(val, dict):
                        res = find_products_recursive(val)
                        if res: return res
                    if isinstance(val, list) and len(val) > 0:
                        return val
                
                for v in data.values():
                    res = find_products_recursive(v)
                    if res: return res
                return None

            raw_products = find_products_recursive(next_data)
            if raw_products:
                for item in raw_products:
                    if not isinstance(item, dict): continue
                    
                    # Handle ItemList wrappers
                    if "item" in item and isinstance(item["item"], dict):
                        target = item["item"]
                    else:
                        target = item
                    
                    if "name" not in target and "name" not in item: continue
                    
                    product = {
                        "name": target.get("name") or item.get("name"),
                        "brand": target.get("brand_name") or target.get("brand") or item.get("brand_name"),
                        "productId": target.get("id") or target.get("sku") or item.get("id"),
                        "currency": "USD",
                        "images": []
                    }
                    
                    price_raw = target.get("price") or item.get("price")
                    if isinstance(price_raw, (int, float)):
                        product["price"] = float(price_raw)
                    else:
                        product["price"] = extract_numeric_price(str(price_raw))
                    
                    url_path = target.get("url") or item.get("url") or ""
                    product["url"] = make_absolute_url(url_path)
                    
                    img_url = target.get("image_url") or target.get("image") or item.get("image_url")
                    if img_url:
                        product["images"].append({
                            "url": make_absolute_url(img_url),
                            "altText": product["name"]
                        })
                    
                    if product["name"]:
                        products.append(product)
        except Exception as e:
            logger.debug(f"Error parsing __NEXT_DATA__: {e}")

    # Strategy 2: DOM-based extraction
    if not products:
        selectors = [
            ".product-grid-item", ".product-card", "[data-testid='product-card']", 
            ".search-result-item", ".product-item", "[class*='ProductCard']", 
            ".product-list > div", "#maincontent .product"
        ]
        items = soup.select(", ".join(selectors))
        for s in items:
            name_el = s.select_one(".product-card_name, [class*='name'], h3, .title, .product-name, [class*='Title']")
            if not name_el: continue
            
            name = name_el.get_text(strip=True)
            price_el = s.select_one(".product-card_price, [class*='price'], .amount, .price, [class*='Price']")
            price_text = price_el.get_text(strip=True) if price_el else ""
            
            link_el = s.select_one("a[href]")
            relative_url = link_el.get("href", "") if link_el else ""
            
            brand_el = s.select_one(".product-card_brand, [class*='brand'], .vendor, .brand, [class*='Brand']")
            brand = brand_el.get_text(strip=True) if brand_el else ""
            
            img_el = s.find("img")
            img_src = ""
            if img_el:
                img_src = img_el.get("src") or ""
                if not img_src or "data:image" in img_src:
                    img_src = img_el.get("data-src", "")
            
            images = []
            if img_src:
                images.append({
                    "url": make_absolute_url(img_src),
                    "altText": name
                })
                
            products.append({
                "name": name,
                "price": extract_numeric_price(price_text),
                "currency": detect_currency(price_text),
                "url": make_absolute_url(relative_url),
                "brand": brand,
                "images": images
            })

    # Strategy 3: JSON-LD Fallback
    if not products:
        ld_scripts = soup.find_all("script", type="application/ld+json")
        for script in ld_scripts:
            try:
                data = json.loads(script.string)
                
                def process_ld(item: Any):
                    if not isinstance(item, dict): return
                    t = item.get("@type")
                    if t == "Product":
                        offer = item.get("offers", {})
                        if isinstance(offer, list): offer = offer[0] if offer else {}
                        
                        products.append({
                            "name": item.get("name"),
                            "productId": item.get("sku"),
                            "price": float(offer.get("price", 0)) if offer.get("price") else 0.0,
                            "currency": offer.get("priceCurrency", "USD"),
                            "url": make_absolute_url(soup.find("link", rel="canonical").get("href", "") if soup.find("link", rel="canonical") else "")
                        })
                    elif t == "ItemList":
                        elements = item.get("itemListElement", [])
                        for sub in elements:
                            if isinstance(sub, dict) and "item" in sub:
                                process_ld(sub["item"])

                if isinstance(data, list):
                    for d in data: process_ld(d)
                else:
                    process_ld(data)
            except Exception:
                continue

    # Metadata
    query_val = "hair shampoo"
    input_q = soup.select_one("input[name='q']")
    if input_q and input_q.get("value"):
        query_val = input_q.get("value")

    metadata = {
        "query": query_val,
        "resultsDisplayed": len(products),
        "searchType": "keyword",
        "searchUrl": "https://www.beautylish.com/shop/browse?utm_source=internal_search&utm_campaign=Search+Beautylish+Products&q=hair+shampoo",
        "totalResults": len(products)
    }

    return ScrapedData(
        products=products,
        searchMetadata=metadata
    )

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data and scraped_data.products:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No products extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/shop/browse?utm_source=internal_search&utm_campaign=Search+Beautylish+Products&q=hair+shampoo",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")