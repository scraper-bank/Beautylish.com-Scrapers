"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import json
import re
import logging
import threading
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_category_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    appliedFilters: List[Dict[str, str]] = field(default_factory=list)
    bannerImage: Optional[str] = None
    breadcrumbs: Optional[Any] = None
    categoryId: str = ""
    categoryName: str = ""
    categoryUrl: str = ""
    description: Optional[str] = None
    pagination: Optional[Any] = None
    products: List[Any] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.get("categoryUrl", str(input_data))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        data_dict = asdict(scraped_data)
        if not self.is_duplicate(data_dict):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(data_dict, ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.cookies": 1,
            "profile.managed_default_content_settings.javascript": 1,
            "profile.managed_default_content_settings.plugins": 2,
            "profile.managed_default_content_settings.popups": 2,
            "profile.managed_default_content_settings.geolocation": 2,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG,
            version_main=None
        )
        
        thread_local.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            """
        })
        
    return thread_local.driver

def strip_html(html_str: str) -> str:
    """Equivalent to Go's stripHTML."""
    if not html_str:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    """Equivalent to Go's makeAbsoluteURL."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://www.beautylish.com"
    if url_str.startswith("/"):
        return domain + url_str
    return domain + "/" + url_str

def detect_currency(price_text: str) -> str:
    """Equivalent to Go's detectCurrency."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def extract_numeric(text: str) -> float:
    """Equivalent to Go's extractNumeric."""
    match = re.search(r'[\d,]+\.?\d*', text)
    if match:
        clean = match.group().replace(",", "")
        try:
            return float(clean)
        except ValueError:
            return 0.0
    return 0.0

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    """Equivalent to Go ExtractData function using Selenium."""
    try:
        # Get canonical URL
        try:
            canon_el = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']")
            canon = canon_el.get_attribute("href") or ""
        except NoSuchElementException:
            canon = url

        applied_filters = []
        if "tag=" in canon:
            parts = canon.split("tag=")
            if len(parts) > 1:
                val = parts[1].split("&")[0]
                applied_filters.append({
                    "filterName": "tag",
                    "filterValue": val
                })

        cat_id = applied_filters[0]["filterValue"] if applied_filters else ""
        
        # Get Category Name from Title
        cat_name = ""
        title_text = driver.title
        if "|" in title_text:
            cat_name = title_text.split("|")[0].strip()
        else:
            cat_name = title_text.strip()

        # Extract Subcategories
        subcategories = []
        sublist_items = driver.find_elements(By.CSS_SELECTOR, ".squeezeBox_sublist .squeezeBox_item a")
        for item in sublist_items:
            try:
                name = item.text.strip()
                href = item.get_attribute("href") or ""
                if name and href:
                    subcategories.append({
                        "name": name,
                        "productCount": None,
                        "url": make_absolute_url(href)
                    })
            except StaleElementReferenceException:
                continue

        return ScrapedData(
            appliedFilters=applied_filters,
            categoryId=cat_id,
            categoryName=cat_name,
            categoryUrl=make_absolute_url(canon),
            subcategories=subcategories,
            bannerImage=None,
            breadcrumbs=None,
            description=None,
            pagination=None,
            products=[]
        )
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            scraped_data = extract_data(driver, url)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url} (Attempt {tries+1}): {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    try:
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
            for future in futures:
                future.result()
    finally:
        if hasattr(thread_local, "driver"):
            thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/shop/browse?tag=skincare",
    ]

    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")