"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import json
import re
import math
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union
import threading

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_search_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    searchMetadata: Dict[str, Any]
    products: List[Dict[str, Any]] = field(default_factory=list)
    breadcrumbs: Optional[Any] = None
    pagination: Optional[Any] = None
    recommendations: Optional[Any] = None
    relatedSearches: Optional[Any] = None
    sponsoredProducts: Optional[Any] = None

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        # Use product IDs or names to check for duplicates within the list
        return False

    def add_data(self, scraped_data: ScrapedData):
        with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
            json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
            output_file.write(json_line + "\n")
        logger.info(f"Saved data to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG,
        )
        
        thread_local.driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": """
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            """
        })
        
    return thread_local.driver

def strip_html(text: str) -> str:
    """Equivalent to Go's stripHTML."""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', text).strip()

def make_absolute_url(url_str: str) -> str:
    """Equivalent to Go's makeAbsoluteURL."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://www.beautylish.com"
    if url_str.startswith("/"):
        return domain + url_str
    return domain + "/" + url_str

def detect_currency(price_text: str) -> str:
    """Equivalent to Go's detectCurrency."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def extract_numeric_price(price_text: str) -> float:
    """Equivalent to Go's extractNumericPrice."""
    if not price_text:
        return 0.0
    price_text = price_text.replace(",", "")
    match = re.search(r'([\d,]+\.?\d*)', price_text)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            pass
    return 0.0

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    """Extract structured data using Selenium, replicating Go logic."""
    products = []
    
    # 1. Extract Search Metadata
    query = ""
    try:
        search_input = driver.find_element(By.CSS_SELECTOR, "input.ocnSearch_input")
        query = search_input.get_attribute("value") or ""
    except NoSuchElementException:
        try:
            title_text = driver.find_element(By.CSS_SELECTOR, "h1.search-title").text
            if '"' in title_text:
                parts = title_text.split('"')
                if len(parts) > 1:
                    query = parts[1]
        except NoSuchElementException:
            pass

    search_url = ""
    try:
        canonical = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']")
        search_url = make_absolute_url(canonical.get_attribute("href"))
    except NoSuchElementException:
        search_url = f"https://www.beautylish.com/shop/browse?q={query.replace(' ', '+')}"

    search_metadata = {
        "query": query.replace(" ", "+"),
        "resultsDisplayed": 0,
        "searchType": "keyword",
        "searchUrl": search_url,
        "totalResults": 0
    }

    # 2. Extract Products from DOM
    selector = ".product-list-container .product-item, .product-grid .product-item, .product-grid-item, .product-card, .product-list-item, [data-product-id], div[data-type='product'], #search-results > div, .products-list > div, .product-list > div"
    containers = driver.find_elements(By.CSS_SELECTOR, selector)
    
    for s in containers:
        try:
            p = {}
            
            # Brand
            brand = ""
            brand_els = s.find_elements(By.CSS_SELECTOR, ".product-brand, .item-brand, .brand, [itemprop='brand'], .brand-name")
            if brand_els:
                brand = brand_els[0].text.strip()
            if not brand:
                brand = s.get_attribute("data-brand") or ""
            
            # Name
            name = ""
            name_els = s.find_elements(By.CSS_SELECTOR, ".product-name, .item-name, h2, .name, .title, [itemprop='name'], .product-title")
            if name_els:
                name = name_els[0].text.strip()
            if not name:
                name = s.get_attribute("data-name") or ""
            
            if brand and brand.lower() not in name.lower():
                p["name"] = f"{brand} {name}"
            else:
                p["name"] = name
            
            # Price
            raw_price = ""
            price_els = s.find_elements(By.CSS_SELECTOR, ".price, .product-price, .item-price, .amount, [itemprop='price'], .price-display")
            if price_els:
                raw_price = price_els[0].text
            if not raw_price:
                raw_price = s.get_attribute("data-price") or ""
            
            p["price"] = extract_numeric_price(raw_price)
            p["currency"] = detect_currency(raw_price)
            
            # URL
            p["url"] = ""
            links = s.find_elements(By.TAG_NAME, "a")
            if links:
                p["url"] = make_absolute_url(links[0].get_attribute("href"))
            elif s.get_attribute("href"):
                p["url"] = make_absolute_url(s.get_attribute("href"))
                
            # ID
            product_id = s.get_attribute("data-product-id") or s.get_attribute("id") or ""
            p["productId"] = product_id
            
            if p.get("name"):
                products.append(p)
        except (StaleElementReferenceException, NoSuchElementException):
            continue

    # 3. Fallback: JSON-LD or Initial State
    if not products:
        scripts = driver.find_elements(By.TAG_NAME, "script")
        for script in scripts:
            try:
                content = script.get_attribute("innerHTML")
                script_type = script.get_attribute("type") or ""
                
                # LD+JSON
                if "application/ld+json" in script_type or "application/ld+json" in content:
                    try:
                        data = json.loads(content.strip())
                        def process_ld(m):
                            if isinstance(m, dict) and "itemListElement" in m:
                                for item in m["itemListElement"]:
                                    if "item" in item:
                                        prod = item["item"]
                                        p = {
                                            "name": prod.get("name"),
                                            "url": make_absolute_url(prod.get("@id", ""))
                                        }
                                        if "offers" in prod:
                                            offers = prod["offers"]
                                            p["price"] = extract_numeric_price(str(offers.get("price", "")))
                                            p["currency"] = str(offers.get("priceCurrency", ""))
                                        products.append(p)
                        
                        if isinstance(data, dict):
                            process_ld(data)
                        elif isinstance(data, list):
                            for item in data: process_ld(item)
                    except json.JSONDecodeError:
                        pass
                
                # Redux / Initial State
                if "window.__INITIAL_STATE__" in content or "window.REDUX_STATE" in content:
                    match = re.search(r'(?:window\.__INITIAL_STATE__|window\.REDUX_STATE)\s*=\s*({.*?});', content)
                    if match:
                        try:
                            state = json.loads(match.group(1))
                            paths = ["results", "products", "items", "search.results", "catalog.products", "search.data.products"]
                            for path in paths:
                                current = state
                                parts = path.split(".")
                                for i, part in enumerate(parts):
                                    if i == len(parts) - 1:
                                        if isinstance(current, dict) and isinstance(current.get(part), list):
                                            for res in current[part]:
                                                if isinstance(res, dict):
                                                    products.append({
                                                        "name": res.get("name"),
                                                        "productId": res.get("id"),
                                                        "url": make_absolute_url(str(res.get("url", "")))
                                                    })
                                    else:
                                        if isinstance(current, dict) and isinstance(current.get(part), dict):
                                            current = current[part]
                                        else:
                                            break
                        except json.JSONDecodeError:
                            pass
            except Exception:
                continue

    search_metadata["resultsDisplayed"] = len(products)
    search_metadata["totalResults"] = len(products)
    
    return ScrapedData(
        searchMetadata=search_metadata,
        products=products
    )

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            scraped_data = extract_data(driver, url)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/shop/browse?utm_source=internal_search&utm_campaign=Search+Beautylish+Products&q=hair+shampoo",
    ]

    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")