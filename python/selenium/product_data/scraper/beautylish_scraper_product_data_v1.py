"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-16
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import re
import logging
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from urllib.parse import urljoin

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"beautylish_com_product_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    aggregateRating: Dict[str, Any] = field(default_factory=lambda: {
        "bestRating": None, "ratingValue": None, "reviewCount": None, "worstRating": None
    })
    availability: str = ""
    brand: str = ""
    category: Optional[str] = None
    currency: str = ""
    description: str = ""
    features: List[str] = field(default_factory=list)
    images: List[Dict[str, str]] = field(default_factory=list)
    name: str = ""
    preDiscountPrice: Optional[float] = None
    price: float = 0.0
    productId: str = ""
    reviews: List[Any] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": None, "rating": None, "url": None
    })
    serialNumbers: List[Any] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    url: str = ""
    videos: List[Dict[str, str]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.productId if hasattr(input_data, 'productId') else str(input_data)
        if not item_key:
            item_key = input_data.url
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance with optimizations."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
        }
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG
        )
    return thread_local.driver

def strip_html(h: str) -> str:
    if not h: return ""
    return re.sub(r'<[^>]*>', ' ', h).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str: return ""
    if url_str.startswith(("http://", "https://")): return url_str
    domain = "https://www.beautylish.com"
    return urljoin(domain, url_str)

def detect_currency(price_text: str) -> str:
    text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR", "GBP": "GBP", "£": "GBP",
        "JPY": "JPY", "¥": "JPY", "CAD": "CAD", "CA$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "CNY": "CNY", "CHF": "CHF"
    }
    for code, currency in currency_map.items():
        if code in text: return currency
    return "USD"

def extract_data(driver: webdriver.Chrome, page_url: str) -> Optional[ScrapedData]:
    """Replicates Go ExtractData logic using Selenium elements and page source."""
    html_source = driver.page_source
    
    # JSON-LD Extraction
    json_data = None
    scripts = driver.find_elements(By.CSS_SELECTOR, "script[type='application/ld+json']")
    for script in scripts:
        try:
            temp = json.loads(script.get_attribute("textContent"))
            if isinstance(temp, dict) and temp.get("@type", "").lower() == "product":
                json_data = temp
                break
        except: continue

    data = ScrapedData()
    data.url = page_url

    # Aggregate Rating
    if json_data and "aggregateRating" in json_data:
        ar = json_data["aggregateRating"]
        for key in ["ratingValue", "reviewCount"]:
            val = ar.get(key)
            try:
                data.aggregateRating[key] = float(val) if val is not None else None
            except: pass

    # Availability
    if json_data and "offers" in json_data:
        avail = json_data["offers"].get("availability", "")
        if "InStock" in avail: data.availability = "in_stock"
        elif "OutOfStock" in avail: data.availability = "out_of_stock"
    
    if not data.availability:
        try:
            status_el = driver.find_element(By.CSS_SELECTOR, ".availability-status")
            status_attr = (status_el.get_attribute("data-availability") or "").lower()
            if "in_stock" in status_attr or "In Stock" in status_el.text:
                data.availability = "in_stock"
        except NoSuchElementException: pass

    # Brand
    if json_data and "brand" in json_data:
        data.brand = json_data["brand"].get("name", "")
    if not data.brand:
        try:
            data.brand = driver.find_element(By.CSS_SELECTOR, ".brand-name").text.strip()
        except NoSuchElementException: pass

    # Currency
    if json_data and "offers" in json_data:
        data.currency = json_data["offers"].get("priceCurrency", "")
    if not data.currency:
        try:
            curr_text = driver.find_element(By.CSS_SELECTOR, ".currency, .price-container").text
            data.currency = detect_currency(curr_text)
        except NoSuchElementException: data.currency = "USD"

    # Description
    if json_data:
        data.description = json_data.get("description", "")
    if not data.description:
        try:
            data.description = driver.find_element(By.CSS_SELECTOR, ".product-description").get_attribute("innerHTML")
        except NoSuchElementException: pass

    # Features
    if data.description:
        feature_part = data.description
        match = re.search(r'(?i)<h[1-6][^>]*>\s*Other Details\s*</h[1-6]>', feature_part)
        if match:
            feature_part = feature_part[:match.start()]
        
        li_matches = re.findall(r'<li>(.*?)</li>', feature_part, re.DOTALL)
        spec_patterns = ["Size:", "Weight:", "Dimensions:", "Gluten-Free", "Dairy-Free", "No Sugar Added"]
        for m in li_matches:
            feat = strip_html(m).replace("&amp;", "&").strip()
            if feat and not any(p in feat for p in spec_patterns):
                data.features.append(feat)

    if not data.features:
        li_elements = driver.find_elements(By.CSS_SELECTOR, ".product-description ul li")
        for li in li_elements:
            # Check if within specifications
            if li.find_elements(By.XPATH, "./ancestor::*[contains(@class, 'product-specifications')]"):
                continue
            txt = li.text.strip()
            if txt and "Size:" not in txt and "Free" not in txt:
                data.features.append(txt)

    # Images
    seen_urls = set()
    def add_image(url, alt):
        abs_url = make_absolute_url(url)
        if abs_url and abs_url not in seen_urls and not any(x in abs_url for x in ["85x85", "35x35"]):
            data.images.append({"url": abs_url, "altText": alt.strip()})
            seen_urls.add(abs_url)

    img_els = driver.find_elements(By.CSS_SELECTOR, "div[data-section='product-carousel'] .transform-gpu img, .product-media img")
    for img in img_els:
        add_image(img.get_attribute("src"), img.get_attribute("alt") or "")

    if not data.images:
        thumb_els = driver.find_elements(By.CSS_SELECTOR, "div[data-section='product-carousel'] button img")
        for img in thumb_els:
            src = img.get_attribute("src") or ""
            high_res = re.sub(r'p_\d+x\d+', 'zb_p', src)
            add_image(high_res, img.get_attribute("alt") or "")

    # Product Name
    if json_data: data.name = json_data.get("name", "")
    if not data.name:
        try:
            data.name = driver.find_element(By.CSS_SELECTOR, "h1.product-name, .product-header h1").text.strip()
        except NoSuchElementException: pass

    # Price
    price_found = False
    if json_data and "offers" in json_data:
        p = json_data["offers"].get("price")
        try:
            data.price = float(p)
            price_found = True
        except: pass
    
    if not price_found:
        try:
            p_text = driver.find_element(By.CSS_SELECTOR, ".price").text.replace(",", "")
            m = re.search(r'[\d.]+', p_text)
            if m: data.price = float(m.group())
        except: pass

    # Product ID
    try:
        canonical = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']").get_attribute("href")
        if canonical:
            data.productId = canonical.rstrip("/").split("/")[-1]
    except NoSuchElementException: pass

    # Seller
    try:
        s_link = driver.find_element(By.CSS_SELECTOR, ".seller-link, .seller-info a, [itemprop='seller'] a")
        data.seller["name"] = s_link.text.strip()
        data.seller["url"] = make_absolute_url(s_link.get_attribute("href"))
    except NoSuchElementException:
        if json_data and "offers" in json_data and "seller" in json_data["offers"]:
            s = json_data["offers"]["seller"]
            data.seller["name"] = s.get("name")
            data.seller["url"] = make_absolute_url(s.get("url", ""))
    
    if not data.seller["name"]:
        try:
            data.seller["name"] = driver.find_element(By.CSS_SELECTOR, "meta[property='og:site_name']").get_attribute("content")
        except NoSuchElementException: pass
    
    if data.seller["name"] and "beautylish" in data.seller["name"].lower():
        data.seller["name"] = "Beautylish"
        if not data.seller["url"]: data.seller["url"] = "https://www.beautylish.com"

    # Specifications
    spec_items = driver.find_elements(By.CSS_SELECTOR, ".product-specifications .spec-item")
    for item in spec_items:
        try:
            k = item.find_element(By.CSS_SELECTOR, ".key").text.strip()
            v = item.find_element(By.CSS_SELECTOR, ".value").text.strip()
        except NoSuchElementException:
            full_text = item.text.strip()
            if ":" in full_text:
                parts = full_text.split(":", 1)
                k, v = parts[0], parts[1]
            else:
                k, v = "Dietary", full_text
        
        if k and v:
            data.specifications.append({"key": k.rstrip(":").strip(), "value": v.strip()})

    # Videos
    seen_vids = set()
    v_selectors = driver.find_elements(By.CSS_SELECTOR, "[data-video-url], video, source")
    for v in v_selectors:
        for attr in ["data-video-url", "src", "data-src"]:
            v_url = v.get_attribute(attr)
            if v_url:
                abs_v = make_absolute_url(v_url)
                if abs_v not in seen_vids:
                    data.videos.append({"url": abs_v})
                    seen_vids.add(abs_v)
                    break

    return data

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            scraped_data = extract_data(driver, url)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
        finally:
            tries += 1

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.beautylish.com/s/vital-proteins-vanilla-collagen-creamer-10-6-oz",
    ]
    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")